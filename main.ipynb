{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Implementation of Pointer networks: http://arxiv.org/pdf/1506.03134v1.pdf.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Implementation of Pointer networks: http://arxiv.org/pdf/1506.03134v1.pdf.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from dataset import DataGenerator\n",
    "from pointer import pointer_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('batch_size', 32, 'Batch size.  ')\n",
    "flags.DEFINE_integer('max_steps', 10, 'Number of numbers to sort.  ')\n",
    "flags.DEFINE_integer('rnn_size', 32, 'RNN size.  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PointerNetwork(object):\n",
    "    def __init__(self, max_len, input_size, size, num_layers, max_gradient_norm, batch_size, learning_rate,\n",
    "                 learning_rate_decay_factor):\n",
    "        \"\"\"Create the network. A simplified network that handles only sorting.\n",
    "        \n",
    "        Args:\n",
    "            max_len: maximum length of the model.\n",
    "            input_size: size of the inputs data.\n",
    "            size: number of units in each layer of the model.\n",
    "            num_layers: number of layers in the model.\n",
    "            max_gradient_norm: gradients will be clipped to maximally this norm.\n",
    "            batch_size: the size of the batches used during training;\n",
    "                the model construction is independent of batch_size, so it can be\n",
    "                changed after initialization if this is convenient, e.g., for decoding.\n",
    "            learning_rate: learning rate to start with.\n",
    "            learning_rate_decay_factor: decay learning rate by this much when needed.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
    "        self.learning_rate_decay_op = self.learning_rate.assign(\n",
    "            self.learning_rate * learning_rate_decay_factor)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        cell = tf.contrib.rnn.GRUCell(size)\n",
    "        dec_cell = tf.contrib.rnn.GRUCell(size)\n",
    "        if num_layers > 1:\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([single_cell] * num_layers)\n",
    "\n",
    "        self.encoder_inputs = []\n",
    "        self.decoder_inputs = []\n",
    "        self.decoder_targets = []\n",
    "        self.target_weights = []\n",
    "        for i in range(max_len):\n",
    "            self.encoder_inputs.append(tf.placeholder(\n",
    "                tf.float32, [batch_size, input_size], name=\"EncoderInput%d\" % i))\n",
    "\n",
    "        for i in range(max_len + 1):\n",
    "            self.decoder_inputs.append(tf.placeholder(\n",
    "                tf.float32, [batch_size, input_size], name=\"DecoderInput%d\" % i))\n",
    "            self.decoder_targets.append(tf.placeholder(\n",
    "                tf.float32, [batch_size, max_len + 1], name=\"DecoderTarget%d\" % i))  # one hot\n",
    "            self.target_weights.append(tf.placeholder(\n",
    "                tf.float32, [batch_size, 1], name=\"TargetWeight%d\" % i))\n",
    "\n",
    "        # Encoder\n",
    "\n",
    "        # Need for attention\n",
    "        encoder_outputs, final_state = tf.contrib.rnn.static_rnn(cell, self.encoder_inputs, dtype=tf.float32)\n",
    "\n",
    "        # Need a dummy output to point on it. End of decoding.\n",
    "        encoder_outputs = [tf.zeros([FLAGS.batch_size, FLAGS.rnn_size])] + encoder_outputs\n",
    "\n",
    "        # First calculate a concatenation of encoder outputs to put attention on.\n",
    "        top_states = [tf.reshape(e, [-1, 1, cell.output_size])\n",
    "                      for e in encoder_outputs]\n",
    "        attention_states = tf.concat(axis=1, values=top_states)\n",
    "\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            outputs, states, _ = pointer_decoder(\n",
    "                self.decoder_inputs, final_state, attention_states, dec_cell)\n",
    "\n",
    "        with tf.variable_scope(\"decoder\", reuse=True):\n",
    "            predictions, _, inps = pointer_decoder(\n",
    "                self.decoder_inputs, final_state, attention_states, dec_cell, feed_prev=True)\n",
    "\n",
    "        self.predictions = predictions\n",
    "\n",
    "        self.outputs = outputs\n",
    "        self.inps = inps\n",
    "        # move code below to a separate function as in TF examples\n",
    "\n",
    "    def create_feed_dict(self, encoder_input_data, decoder_input_data, decoder_target_data):\n",
    "        feed_dict = {}\n",
    "        for placeholder, data in zip(self.encoder_inputs, encoder_input_data):\n",
    "            feed_dict[placeholder] = data\n",
    "\n",
    "        for placeholder, data in zip(self.decoder_inputs, decoder_input_data):\n",
    "            feed_dict[placeholder] = data\n",
    "\n",
    "        for placeholder, data in zip(self.decoder_targets, decoder_target_data):\n",
    "            feed_dict[placeholder] = data\n",
    "\n",
    "        for placeholder in self.target_weights:\n",
    "            feed_dict[placeholder] = np.ones([self.batch_size, 1])\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def step(self):\n",
    "\n",
    "        loss = 0.0\n",
    "        for output, target, weight in zip(self.outputs, self.decoder_targets, self.target_weights):\n",
    "            loss += tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=target) * weight\n",
    "\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        test_loss = 0.0\n",
    "        for output, target, weight in zip(self.predictions, self.decoder_targets, self.target_weights):\n",
    "            test_loss += tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=target) * weight\n",
    "\n",
    "        test_loss = tf.reduce_mean(test_loss)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op = optimizer.minimize(loss)\n",
    "\n",
    "        train_loss_value = 0.0\n",
    "        test_loss_value = 0.0\n",
    "\n",
    "        correct_order = 0\n",
    "        all_order = 0\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            merged = tf.summary.merge_all()\n",
    "            writer = tf.summary.FileWriter(\"/tmp/pointer_logs\", sess.graph)\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "            for i in range(100000):\n",
    "                encoder_input_data, decoder_input_data, targets_data = dataset.next_batch(\n",
    "                    FLAGS.batch_size, FLAGS.max_steps)\n",
    "\n",
    "                # Train\n",
    "                feed_dict = self.create_feed_dict(\n",
    "                    encoder_input_data, decoder_input_data, targets_data)\n",
    "                d_x, l = sess.run([loss, train_op], feed_dict=feed_dict)\n",
    "                train_loss_value = 0.9 * train_loss_value + 0.1 * d_x\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print('Step: %d' % i)\n",
    "                    print(\"Train: \", train_loss_value)\n",
    "\n",
    "                encoder_input_data, decoder_input_data, targets_data = dataset.next_batch(\n",
    "                    FLAGS.batch_size, FLAGS.max_steps, train_mode=False)\n",
    "                # Test\n",
    "                feed_dict = self.create_feed_dict(\n",
    "                    encoder_input_data, decoder_input_data, targets_data)\n",
    "                inps_ = sess.run(self.inps, feed_dict=feed_dict)\n",
    "\n",
    "                predictions = sess.run(self.predictions, feed_dict=feed_dict)\n",
    "\n",
    "                test_loss_value = 0.9 * test_loss_value + 0.1 * sess.run(test_loss, feed_dict=feed_dict)\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(\"Test: \", test_loss_value)\n",
    "\n",
    "                predictions_order = np.concatenate([np.expand_dims(prediction, 0) for prediction in predictions])\n",
    "                predictions_order = np.argmax(predictions_order, 2).transpose(1, 0)[:, 0:FLAGS.max_steps]\n",
    "\n",
    "                input_order = np.concatenate(\n",
    "                    [np.expand_dims(encoder_input_data_, 0) for encoder_input_data_ in encoder_input_data])\n",
    "                input_order = np.argsort(input_order, 0).squeeze().transpose(1, 0) + 1\n",
    "\n",
    "                correct_order += np.sum(np.all(predictions_order == input_order,\n",
    "                                               axis=1))\n",
    "                all_order += FLAGS.batch_size\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print('Correct order / All order: %f' % (correct_order / all_order))\n",
    "                    correct_order = 0\n",
    "                    all_order = 0\n",
    "\n",
    "                    # print(encoder_input_data, decoder_input_data, targets_data)\n",
    "                    # print(inps_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Train:  2.63831939697\n",
      "Test:  2.63792362213\n",
      "Correct order / All order: 0.000000\n",
      "Step: 100\n",
      "Train:  24.8947001654\n",
      "Test:  24.8892710905\n",
      "Correct order / All order: 0.000000\n",
      "Step: 200\n",
      "Train:  21.2497463536\n",
      "Test:  21.4848275633\n",
      "Correct order / All order: 0.000000\n",
      "Step: 300\n",
      "Train:  16.7838904604\n",
      "Test:  17.6563862097\n",
      "Correct order / All order: 0.000000\n",
      "Step: 400\n",
      "Train:  13.6685996253\n",
      "Test:  14.9726437882\n",
      "Correct order / All order: 0.000313\n",
      "Step: 500\n",
      "Train:  11.8289969333\n",
      "Test:  13.377557766\n",
      "Correct order / All order: 0.000313\n",
      "Step: 600\n",
      "Train:  10.5324264561\n",
      "Test:  11.6975769636\n",
      "Correct order / All order: 0.000313\n",
      "Step: 700\n",
      "Train:  10.2349293053\n",
      "Test:  11.0991678343\n",
      "Correct order / All order: 0.001250\n",
      "Step: 800\n",
      "Train:  9.94064353096\n",
      "Test:  11.0119140893\n",
      "Correct order / All order: 0.005000\n",
      "Step: 900\n",
      "Train:  9.63068029371\n",
      "Test:  10.2355568124\n",
      "Correct order / All order: 0.005938\n",
      "Step: 1000\n",
      "Train:  9.29692774384\n",
      "Test:  10.0402815509\n",
      "Correct order / All order: 0.005938\n",
      "Step: 1100\n",
      "Train:  8.79019164459\n",
      "Test:  9.8560653588\n",
      "Correct order / All order: 0.008438\n",
      "Step: 1200\n",
      "Train:  8.54483655262\n",
      "Test:  9.31971382617\n",
      "Correct order / All order: 0.012188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0efe0f79828d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpointer_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPointerNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpointer_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-5e076857e6b1>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 feed_dict = self.create_feed_dict(\n\u001b[1;32m    122\u001b[0m                     encoder_input_data, decoder_input_data, targets_data)\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0md_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m                 \u001b[0mtrain_loss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain_loss_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luciodery/Desktop/stanford/spring2017/cs231n/assignment1/.env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luciodery/Desktop/stanford/spring2017/cs231n/assignment1/.env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luciodery/Desktop/stanford/spring2017/cs231n/assignment1/.env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/luciodery/Desktop/stanford/spring2017/cs231n/assignment1/.env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luciodery/Desktop/stanford/spring2017/cs231n/assignment1/.env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pointer_network = PointerNetwork(FLAGS.max_steps, 1, FLAGS.rnn_size, 1, 5, FLAGS.batch_size, 1e-2, 0.95)\n",
    "dataset = DataGenerator()\n",
    "pointer_network.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
